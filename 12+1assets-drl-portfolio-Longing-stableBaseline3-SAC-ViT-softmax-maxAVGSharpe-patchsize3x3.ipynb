{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# plotting\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/envs/sblines3/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "window_length = 59\n",
    "steps = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dir\n",
    "save_dir = \"./outputs/pytorch-DDPG/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rl_portfolio_management.environments.portfolio import PortfolioEnv\n",
    "from rl_portfolio_management.callbacks.notebook_plot import LivePlotNotebook\n",
    "from rl_portfolio_management.config import eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_portfolio_management.util import sharpe\n",
    "\n",
    "class DataSrc(object):\n",
    "    \"\"\"Acts as data provider for each new episode.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 df,\n",
    "                 steps=252, \n",
    "                 scale=True, scale_extra_cols=True, augment=0.00, window_length=50, random_reset=True):\n",
    "        \"\"\"\n",
    "        DataSrc.\n",
    "\n",
    "        df - csv for data frame index of timestamps\n",
    "             and multi-index columns levels=[['LTCBTC'],...],['open','low','high','close',...]]\n",
    "             an example is included as an hdf file in this repository\n",
    "        steps - total steps in episode\n",
    "        scale - scale the data for each episode\n",
    "        scale_extra_cols - scale extra columns by global mean and std\n",
    "        augment - fraction to augment the data by\n",
    "        random_reset - reset to a random time (otherwise continue through time)\n",
    "        \"\"\"\n",
    "        self.steps = steps + 1\n",
    "        self.augment = augment\n",
    "        self.random_reset = random_reset\n",
    "        self.scale = scale\n",
    "        self.scale_extra_cols = scale_extra_cols\n",
    "        self.window_length = window_length\n",
    "        self.idx = self.window_length\n",
    "        \n",
    "\n",
    "\n",
    "        # get rid of NaN's\n",
    "        df = df.copy()\n",
    "        df.replace(np.nan, 0, inplace=True)\n",
    "        df = df.fillna(method=\"pad\")\n",
    "\n",
    "        # dataframe to matrix\n",
    "        self.asset_names = df.columns.levels[0].tolist()\n",
    "        self.features = df.columns.levels[1].tolist()\n",
    "        data = df.values.reshape(\n",
    "            (len(df), len(self.asset_names), len(self.features)))\n",
    "        self._data = np.transpose(data, (1, 0, 2))\n",
    "        self._times = df.index\n",
    "\n",
    "        self.price_columns = ['close', 'high', 'low']\n",
    "#         self.price_columns = ['close', 'high', 'low', 'open']\n",
    "        self.non_price_columns = set(\n",
    "            df.columns.levels[1]) - set(self.price_columns)\n",
    "\n",
    "        # Stats to let us normalize non price columns\n",
    "        if scale_extra_cols:\n",
    "            x = self._data.reshape((-1, len(self.features)))\n",
    "            self.stats = dict(mean=x.mean(0), std=x.std(0))\n",
    "            # for column in self._data.columns.levels[1].tolist():\n",
    "            #     x = df.xs(key=column, axis=1, level='Price').as_matrix()[:, :]\n",
    "            #     self.stats[\"mean\"].append(x.mean())\n",
    "            #      = dict(mean=x.mean(), std=x.std())\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _step(self):\n",
    "        # get history matrix from dataframe\n",
    "        data_window = self.data[:, self.step:self.step +\n",
    "                                self.window_length].copy()\n",
    "        # (eq.1) prices\n",
    "        y1 = data_window[:, -1, 0] / data_window[:, -2, 0]\n",
    "        y1 = np.concatenate([[1.0], y1])  # add cash price\n",
    "\n",
    "        # (eq 18) X: prices are divided by close price\n",
    "        nb_pc = len(self.price_columns)\n",
    "        if self.scale:\n",
    "            last_close_price = data_window[:, -1, 0]\n",
    "            data_window[:, :, :nb_pc] /= last_close_price[:,\n",
    "                                                          np.newaxis, np.newaxis]\n",
    "\n",
    "        if self.scale_extra_cols:\n",
    "            # normalize non price columns\n",
    "            data_window[:, :, nb_pc:] -= self.stats[\"mean\"][None, None, nb_pc:]\n",
    "            data_window[:, :, nb_pc:] /= self.stats[\"std\"][None, None, nb_pc:]\n",
    "            data_window[:, :, nb_pc:] = np.clip(\n",
    "                data_window[:, :, nb_pc:],\n",
    "                self.stats[\"mean\"][nb_pc:] - self.stats[\"std\"][nb_pc:] * 10,\n",
    "                self.stats[\"mean\"][nb_pc:] + self.stats[\"std\"][nb_pc:] * 10\n",
    "            )\n",
    "            data_window[np.isinf(data_window)] = 0\n",
    "            data_window[np.isnan(data_window)] = 0\n",
    "\n",
    "        self.step += 1\n",
    "        history = data_window\n",
    "        done = bool(self.step >= self.steps)\n",
    "\n",
    "        return history, y1, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "        # get data for this episode\n",
    "        if self.random_reset:\n",
    "            self.idx = np.random.randint(\n",
    "                low=self.window_length + 1, high=self._data.shape[1] - self.steps - 2)\n",
    "        else:\n",
    "            # continue sequentially, before reseting to start\n",
    "            if self.idx>(self._data.shape[1] - self.steps - self.window_length - 1):\n",
    "                self.idx=self.window_length + 1\n",
    "            else:\n",
    "                self.idx += self.steps\n",
    "        data = self._data[:, self.idx -\n",
    "                          self.window_length:self.idx + self.steps + 1].copy()\n",
    "        self.times = self._times[self.idx -\n",
    "                                 self.window_length:self.idx + self.steps + 1]\n",
    "\n",
    "        # augment data to prevent overfitting\n",
    "        data += np.random.normal(loc=0, scale=self.augment, size=data.shape)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "class PortfolioSim(object):\n",
    "    \"\"\"\n",
    "    Portfolio management sim.\n",
    "\n",
    "    Params:\n",
    "    - cost e.g. 0.0025 is max in Poliniex\n",
    "\n",
    "    Based of [Jiang 2017](https://arxiv.org/abs/1706.10059)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, asset_names=[], steps=128, trading_cost=0.0025, time_cost=0.0):\n",
    "        self.cost = trading_cost\n",
    "        self.time_cost = time_cost\n",
    "        self.steps = steps\n",
    "        self.asset_names = asset_names\n",
    "        self.reset()\n",
    "\n",
    "    def _step(self, w1, y1):\n",
    "        \"\"\"\n",
    "        Step.\n",
    "\n",
    "        w1 - new action of portfolio weights - e.g. [0.1,0.9, 0.0]\n",
    "        y1 - price relative vector also called return\n",
    "            e.g. [1.0, 0.9, 1.1]\n",
    "        Numbered equations are from https://arxiv.org/abs/1706.10059\n",
    "        \"\"\"\n",
    "        w0 = self.w0\n",
    "        p0 = self.p0\n",
    "        #\n",
    "        # print(\"y1:\",y1)\n",
    "        # print(\"w1\",w1)\n",
    "        dw1 = (y1 * w0) / (np.dot(y1, w0) + eps)  # (eq7) weights evolve into\n",
    "        # print(\"w0:\",w0)\n",
    "        # (eq16) cost to change portfolio\n",
    "        # (excluding change in cash to avoid double counting for transaction cost)\n",
    "        c1 = self.cost * (\n",
    "            np.abs(dw1[1:] - w1[1:])).sum()\n",
    "\n",
    "        p1 = p0 * (1 - c1) * np.exp(np.dot(np.log(y1), w0))  # (eq11) final portfolio value\n",
    "\n",
    "        p1 = p1 * (1 - self.time_cost)  # we can add a cost to holding\n",
    "\n",
    "        # can't have negative holdings in this model (no shorts)\n",
    "        # p1 = np.clip(p1, 0, np.inf)\n",
    "\n",
    "        rho1 = p1 / p0 - 1  # rate of returns\n",
    "        r1 = np.log((p1 + eps) / (p0 + eps))  # (eq10) log rate of return\n",
    "        # r1 = np.log(1 - c1) + np.dot(np.log(y1), w0)  # (eq10) log rate of return\n",
    "        # (eq22) immediate reward is log rate of return scaled by episode length\n",
    "\n",
    "        self.ret.append(r1) # 把收益率保存在容器中\n",
    "\n",
    "        reward = sharpe(np.array(self.ret)) / self.steps\n",
    "        # reward = r1 / self.steps\n",
    "        # reward = np.power(r1, 1/self.steps)\n",
    "\n",
    "        # remember for next step\n",
    "        self.w0 = w1\n",
    "        self.p0 = p1\n",
    "\n",
    "\n",
    "        # if we run out of money, we're done\n",
    "        done = bool(p1 == 0)\n",
    "\n",
    "        # should only return single values, not list\n",
    "        info = {\n",
    "            \"reward\": reward,\n",
    "            \"log_return\": r1,\n",
    "            \"portfolio_value\": p1,\n",
    "            \"market_return\": y1.mean(),\n",
    "            \"rate_of_return\": rho1,\n",
    "            \"weights_mean\": w1.mean(),\n",
    "            \"weights_std\": w1.std(),\n",
    "            \"cost\": c1,\n",
    "        }\n",
    "        # record weights and prices\n",
    "        for i, name in enumerate(['CASH'] + self.asset_names):\n",
    "            info['weight_' + name] = w1[i]\n",
    "            info['price_' + name] = y1[i]\n",
    "\n",
    "        self.infos.append(info)\n",
    "        return reward, info, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.infos = []\n",
    "        self.w0 = np.array([1.0] + [0.0] * len(self.asset_names))\n",
    "        self.p0 = 1.0\n",
    "        self.ret = [] # 定义一个容器，保存收益率，用作sharpe比率计算\n",
    "\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    An environment for financial portfolio management.\n",
    "\n",
    "    Financial portfolio management is the process of constant redistribution of a fund into different\n",
    "    financial products.\n",
    "\n",
    "    Based on [Huang 2020](https://arxiv.org/abs/2012.13773)\n",
    "    \"\"\"\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {'render.modes': ['notebook', 'ansi']}\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 steps=256,\n",
    "                 trading_cost=0.0025,\n",
    "                 time_cost=0.00,\n",
    "                 window_length=window_length,\n",
    "                 augment=0.00,\n",
    "                 output_mode='EIIE',\n",
    "                 log_dir=None,\n",
    "                 scale=True,\n",
    "                 scale_extra_cols=True,\n",
    "                 random_reset=True\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        An environment for financial portfolio management.\n",
    "\n",
    "        Params:\n",
    "            df - csv for data frame index of timestamps\n",
    "                 and multi-index columns levels=[['LTCBTC'],...],['open','low','high','close']]\n",
    "            steps - steps in episode\n",
    "            window_length - how many past observations[\"history\"] to return\n",
    "            trading_cost - cost of trade as a fraction,  e.g. 0.0025 corresponding to max rate of 0.25% at Poloniex (2017)\n",
    "            time_cost - cost of holding as a fraction\n",
    "            augment - fraction to randomly shift data by\n",
    "            output_mode: decides observation[\"history\"] shape\n",
    "            - 'EIIE' for (assets, window, 3)\n",
    "            - 'atari' for (window, window, 3) (assets is padded)\n",
    "            - 'mlp' for (assets*window*3)\n",
    "            log_dir: directory to save plots to\n",
    "            scale - scales price data by last opening price on each episode (except return)\n",
    "            scale_extra_cols - scales non price data using mean and std for whole dataset\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        self.src = DataSrc(df=df, steps=steps, scale=scale, scale_extra_cols=scale_extra_cols,\n",
    "                           augment=augment, window_length=window_length,\n",
    "                           random_reset=random_reset)\n",
    "        self._plot = self._plot2 = self._plot3 = None\n",
    "        self.output_mode = output_mode\n",
    "        self.sim = PortfolioSim(\n",
    "            asset_names=self.src.asset_names,\n",
    "            trading_cost=trading_cost,\n",
    "            time_cost=time_cost,\n",
    "            steps=steps)\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        # openai gym attributes\n",
    "        # action will be the portfolio weights [cash_bias,w1,w2...] where wn are [-1, 1] for each asset\n",
    "        nb_assets = len(self.src.asset_names)\n",
    "        self.action_space = gym.spaces.Box(0, 1.0, shape=(nb_assets + 1,), dtype=np.float32)\n",
    "        \n",
    "        # get the history space from the data min and max\n",
    "        if output_mode == 'EIIE':\n",
    "            obs_shape = (\n",
    "                nb_assets,\n",
    "                window_length,\n",
    "                len(self.src.features)\n",
    "            )\n",
    "            \n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            'history': gym.spaces.Box(\n",
    "                -10,\n",
    "                20 if scale else 1,  # if scale=True observed price changes return could be large fractions\n",
    "                obs_shape\n",
    "            ),\n",
    "            'weights': self.action_space    \n",
    "        })\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sim.reset()\n",
    "        self.src.reset()\n",
    "        self.infos = []\n",
    "        action = self.sim.w0\n",
    "        observation, reward, done, info = self.step(action)\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Step the env.\n",
    "\n",
    "        Actions should be portfolio [w0...]\n",
    "        - Where wn is a portfolio weight between 0 and 1. The first (w0) is cash_bias\n",
    "        - cn is the portfolio conversion weights see PortioSim._step for description\n",
    "        \"\"\"\n",
    "        logger.debug('action: %s', action)\n",
    "#         weights = np.clip(action, -1.0, 1.0)\n",
    "#         weights[0] = np.clip(weights[0], 0, 1)\n",
    "#         weights_abs = np.abs(weights)\n",
    "#         weights /= weights_abs.sum() + eps\n",
    "        # print('weights:',weights)\n",
    "        # weights = action\n",
    "        weights = action\n",
    "        \n",
    "#         weights /= weights.sum() \n",
    "\n",
    "\n",
    "        # Sanity checks\n",
    "        assert self.action_space.contains(\n",
    "            action), 'action should be within %r but is %r' % (self.action_space, action)\n",
    "        np.testing.assert_almost_equal(\n",
    "            np.sum(weights), 1.0, 3, err_msg='weights should sum to 1. action=\"%s\"' % weights)\n",
    "\n",
    "        history, y1, done1 = self.src._step()\n",
    "\n",
    "        reward, info, done2 = self.sim._step(weights, y1)\n",
    "\n",
    "        # calculate return for buy and hold a bit of each asset\n",
    "        info['market_value'] = np.cumprod(\n",
    "            [inf[\"market_return\"] for inf in self.infos + [info]])[-1]\n",
    "        # add dates\n",
    "        info['date'] = self.src.times[self.src.step].timestamp()\n",
    "        info['steps'] = self.src.step\n",
    "\n",
    "        self.infos.append(info)\n",
    "\n",
    "        # reshape history according to output mode\n",
    "        if self.output_mode == 'EIIE':\n",
    "            pass\n",
    "        elif self.output_mode == 'atari':\n",
    "            padding = history.shape[1] - history.shape[0]\n",
    "            history = np.pad(history, [[0, padding], [\n",
    "                0, 0], [0, 0]], mode='constant')\n",
    "        elif self.output_mode == 'mlp':\n",
    "            history = history.flatten()\n",
    "\n",
    "        return {'history': history, 'weights': weights}, reward, done1 or done2, info\n",
    "    \n",
    "    def _seed(self, seed):\n",
    "        np.random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def render(self, mode='notebook', close=False):\n",
    "        # if close:\n",
    "            # return\n",
    "        if mode == 'ansi':\n",
    "            pprint(self.infos[-1])\n",
    "        elif mode == 'notebook':\n",
    "            self.plot_notebook(close)\n",
    "\n",
    "    def plot_notebook(self, close=False):\n",
    "        \"\"\"Live plot using the jupyter notebook rendering of matplotlib.\"\"\"\n",
    "\n",
    "        if close:\n",
    "            self._plot = self._plot2 = self._plot3 = None\n",
    "            return\n",
    "\n",
    "        df_info = pd.DataFrame(self.infos)\n",
    "        df_info.index = pd.to_datetime(df_info[\"date\"], unit='s')\n",
    "\n",
    "        # plot prices and performance\n",
    "        all_assets = ['CASH'] + self.sim.asset_names\n",
    "        if not self._plot:\n",
    "            colors = [None] * len(all_assets) + ['black']\n",
    "            self._plot_dir = os.path.join(\n",
    "                self.log_dir, 'notebook_plot_prices_' + str(time.time())) if self.log_dir else None\n",
    "            self._plot = LivePlotNotebook(\n",
    "                log_dir=self._plot_dir, title='prices & performance', labels=all_assets + [\"Portfolio\"], ylabel='value', colors=colors)\n",
    "        x = df_info.index\n",
    "        y_portfolio = df_info[\"portfolio_value\"]\n",
    "        y_assets = [df_info['price_' + name].cumprod()\n",
    "                    for name in all_assets]\n",
    "        self._plot.update(x, y_assets + [y_portfolio])\n",
    "\n",
    "\n",
    "        # plot portfolio weights\n",
    "        if not self._plot2:\n",
    "            self._plot_dir2 = os.path.join(\n",
    "                self.log_dir, 'notebook_plot_weights_' + str(time.time())) if self.log_dir else None\n",
    "            self._plot2 = LivePlotNotebook(\n",
    "                log_dir=self._plot_dir2, labels=all_assets, title='weights', ylabel='weight')\n",
    "        ys = [df_info['weight_' + name] for name in all_assets]\n",
    "        self._plot2.update(x, ys)\n",
    "\n",
    "        # plot portfolio costs\n",
    "        if not self._plot3:\n",
    "            self._plot_dir3 = os.path.join(\n",
    "                self.log_dir, 'notebook_plot_cost_' + str(time.time())) if self.log_dir else None\n",
    "            self._plot3 = LivePlotNotebook(\n",
    "                log_dir=self._plot_dir3, labels=['cost'], title='costs', ylabel='cost')\n",
    "        ys = [df_info['cost']]\n",
    "        self._plot3.update(x, ys)\n",
    "\n",
    "        if close:\n",
    "            self._plot = self._plot2 = self._plot3 = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_portfolio_management.util import MDD, sharpe, softmax, MDD1, sortino, calmar, other_metrics\n",
    "from rl_portfolio_management.wrappers import SoftmaxActions, TransposeHistory, ConcatStates\n",
    "\n",
    "df_train = pd.read_hdf('./data/chinaStock_1d_vol.hf',key='train')\n",
    "df_test = pd.read_hdf('./data/chinaStock_1d_vol.hf',key='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "class DeepRLWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.render_on_reset = False\n",
    "        \n",
    "        self.state_dim = self.observation_space.shape\n",
    "        self.action_dim = self.action_space.shape[0]\n",
    "        \n",
    "        self.name = 'PortfolioEnv'\n",
    "        self.success_threshold = 2\n",
    "        \n",
    "    def normalize_state(self, state):\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info =self.env.step(action)\n",
    "#         reward*=1e4 # often reward scaling is important sooo...\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self):        \n",
    "        # here's a roundabout way to get it to plot on reset\n",
    "        if self.render_on_reset: \n",
    "            self.env.render('notebook')\n",
    "\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(df=df_train, steps=steps, output_mode='EIIE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 60, 12), (6, 60, 12))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def task_fn():\n",
    "    env = PortfolioEnv(df=df_train, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "\n",
    "def task_fn_test(df=df_test, steps=steps):\n",
    "    env = PortfolioEnv(df=df_test, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "    \n",
    "# sanity check\n",
    "task = task_fn()\n",
    "task.reset().shape, task.step(task.action_space.sample())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([[[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]], [[[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]], (6, 60, 12), float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the env\n",
    "env = task_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = task_fn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([[[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]\n",
       "\n",
       " [[-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  ...\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]\n",
       "  [-10. -10. -10. ... -10. -10. -10.]]], [[[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]\n",
       "\n",
       " [[10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  ...\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]\n",
       "  [10. 10. 10. ... 10. 10. 10.]]], (6, 60, 12), float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py:27: UserWarning: It seems that your observation  is an image but the `dtype` of your observation_space is not `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  f\"It seems that your observation {key} is an image but the `dtype` \"\n",
      "/home/andy/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py:35: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  f\"It seems that your observation space {key} is an image but the \"\n",
      "/home/andy/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py:48: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom feature extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  \"The minimal resolution for an image is 36x36 for the default `CnnPolicy`. \"\n",
      "/home/andy/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py:273: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_env(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(task.reset().shape[0])\n",
    "# num_act = task.reset().shape[2] \n",
    "env.action_space.shape[-1] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT网络结构部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 256\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = task.reset().shape[0], patch_size: int = 3, emb_size: int = emb_size):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=self.patch_size, stride=self.patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.h = window_length + 1\n",
    "        self.w = env.action_space.shape[-1] - 1\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        #位置参数，大小为 (数据高/ patch_size) * (数据宽/ patch_size) \n",
    "        self.positions = nn.Parameter(torch.randn((self.h//self.patch_size * self.w//self.patch_size  + 1, emb_size)))\n",
    "\n",
    "                                      \n",
    "    def forward(self, x: Tensor) -> Tensor:                               \n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = emb_size, num_heads: int = 32, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values  = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int=emb_size, expansion: int = 4, drop_p: float = 0):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "#             nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = emb_size,\n",
    "                 drop_p: float = 0,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = emb_size, n_classes: int = emb_size):\n",
    "        n_classes = env.action_space.shape[-1] - 1\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "#             nn.Linear(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels: int = task.reset().shape[0],\n",
    "                patch_size: int = 3,\n",
    "                emb_size: int = emb_size,\n",
    "                depth: int = 6,\n",
    "                n_classes: int = emb_size,\n",
    "                **kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size),\n",
    "            *(TransformerEncoderBlock(emb_size=emb_size, **kwargs) for _ in range(depth)),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sb3中自定义网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "# import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "# from rl_portfolio_management.customExtractor import CustomCNN\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 observation_space: gym.spaces.Box, \n",
    "                 features_dim: int = emb_size):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper       \n",
    "        n_input_channels = observation_space.shape[0]\n",
    "#         num_act = observation_space.shape[2]\n",
    "\n",
    "        self.vit = ViT()\n",
    "        # Compute shape by doing one forward pass\n",
    "#         with th.no_grad():\n",
    "#             n_flatten = self.cnn(\n",
    "#                 th.as_tensor(observation_space.sample()[None]).float()\n",
    "#             ).shape[1]\n",
    "            \n",
    "        with th.no_grad():\n",
    "            n_flatten = self.vit(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "        \n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim, bias=True), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "\n",
    "        x=self.vit(observations)\n",
    "               \n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.policies import BaseModel, BasePolicy, register_policy\n",
    "\n",
    "# from stable_baselines3.common.policies import BaseModel, BasePolicy, create_sde_features_extractor, register_policy\n",
    "# from stable_baselines3.td3.policies import Actor, TD3Policy\n",
    "# from stable_baselines3.common.preprocessing import get_action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomNetwork(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Custom network for policy and value function.\n",
    "#     It receives as input the features extracted by the feature extractor.\n",
    "\n",
    "#     :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "#     :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "#     :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         feature_dim: int,\n",
    "#         last_layer_dim_pi: int = 64,\n",
    "#         last_layer_dim_vf: int = 64,\n",
    "#     ):\n",
    "#         super(CustomNetwork, self).__init__()\n",
    "        \n",
    "#         # IMPORTANT:\n",
    "#         # Save output dimensions, used to create the distributions\n",
    "#         self.latent_dim_pi = last_layer_dim_pi\n",
    "#         self.latent_dim_vf = last_layer_dim_vf\n",
    "#         dropout_half = nn.Dropout(p=0.5)\n",
    "\n",
    "#         # Policy network\n",
    "#         self.policy_net = nn.Sequential(\n",
    "#             nn.Linear(feature_dim, 256), \n",
    "#             nn.ReLU(),\n",
    "#             dropout_half,\n",
    "#             nn.Linear(256, 128), \n",
    "#             nn.ReLU(),\n",
    "#             dropout_half,\n",
    "#             nn.Linear(128, last_layer_dim_pi), \n",
    "#         )\n",
    "#         # Value network\n",
    "#         self.value_net = nn.Sequential(\n",
    "#             nn.Linear(feature_dim, 128), \n",
    "#             nn.ReLU(),\n",
    "#             dropout_half,\n",
    "#             nn.Linear(128, 64), \n",
    "#             nn.ReLU(),\n",
    "#             dropout_half,\n",
    "#             nn.Linear(64, last_layer_dim_vf), \n",
    "#         )\n",
    "\n",
    "#     def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "#         \"\"\"\n",
    "#         :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "#             If all layers are shared, then ``latent_policy == latent_value``\n",
    "#         \"\"\"\n",
    "#         return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "\n",
    "# class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         observation_space: gym.spaces.Space,\n",
    "#         action_space: gym.spaces.Space,\n",
    "#         lr_schedule: Callable[[float], float],\n",
    "#         net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "#         activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "#         *args,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "\n",
    "#         super(CustomActorCriticPolicy, self).__init__(\n",
    "#             observation_space,\n",
    "#             action_space,\n",
    "#             lr_schedule,\n",
    "#             net_arch,\n",
    "#             activation_fn,\n",
    "#             # Pass remaining arguments to base class\n",
    "#             *args,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "#         # Disable orthogonal initialization\n",
    "#         self.ortho_init = False\n",
    "\n",
    "#     def _build_mlp_extractor(self) -> None:\n",
    "#         self.mlp_extractor = CustomNetwork(self.features_dim,\n",
    "#                                            last_layer_dim_pi = env.action_space.shape[-1],\n",
    "#                                            last_layer_dim_vf = 1,\n",
    "#                                           )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.ones(n_actions)*0, sigma=0.27*np.ones(n_actions))\n",
    "# lr_schedule = [[4e-5],5e-4]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=emb_size),\n",
    "#     net_arch=[dict(pi=[256],vf=[256])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs,\n",
    "#             tensorboard_log=\"./runs/ppo-vit1-softmax\",\n",
    "# #             learning_rate=2e-4,\n",
    "#             batch_size=256,\n",
    "#             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", env, policy_kwargs=policy_kwargs,\n",
    "            tensorboard_log=\"./runs/SAC-vit-softmax-maxSharpe\",\n",
    "#             learning_rate=3e-4,\n",
    "            batch_size=256,\n",
    "            action_noise=action_noise, \n",
    "            verbose=1\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TD3(CustomTD3Policy, env, policy_kwargs=policy_kwargs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./runs/SAC-vit-softmax-maxSharpe/SAC_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | -0.947   |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 512      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.2    |\n",
      "|    critic_loss     | 2.46     |\n",
      "|    ent_coef        | 0.884    |\n",
      "|    ent_coef_loss   | -2.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 411      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | -0.0556  |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 2        |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 1024     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -35.7    |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.758    |\n",
      "|    ent_coef_loss   | -6.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 923      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3564/4238477448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4500000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/sac/sac.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         )\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/sac/sac.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# Action by the current actor for the sampled state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mactions_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/sac/policies.py\u001b[0m in \u001b[0;36maction_log_prob\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_dist_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# return action and associated log prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob_from_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mlog_prob_from_params\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob_from_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_actions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_from_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mactions_from_params\u001b[0;34m(self, mean_actions, log_std, deterministic)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactions_from_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_actions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# Update the proba distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_actions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"SquashedDiagGaussianDistribution\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSquashedDiagGaussianDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[1;32m    151\u001b[0m         \u001b[0maction_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sblines3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy_property\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(4500000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modellog_dir = \"./modelsave/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modellog_dir + \"sac-vit1-softmax-maxSharpe-patch3x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agent\n",
    "model_loaded = SAC.load(modellog_dir + \"sac-vit1-softmax-maxSharpe-patch3x3\", env=env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test = env_test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(steps):\n",
    "    action, _states = model_loaded.predict(obs_test)\n",
    "    obs, rewards, dones, info = env_test.step(action)\n",
    "#     env_test.render('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(env_test.infos)\n",
    "df.index = pd.to_datetime(df['date']*1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=sharpe(df.log_return)\n",
    "mdd=MDD(df.portfolio_value)\n",
    "mdd1=MDD1(df.portfolio_value)\n",
    "s1=sharpe(df.rate_of_return)\n",
    "sor_log = sortino(df.log_return)\n",
    "sor_simpe = sortino(df.rate_of_return)\n",
    "calmar_log = calmar(df.log_return.values)\n",
    "\n",
    "# print('回测期日平均简单收益率(average simple return):                 \\t{: 2.6f}'.format( df.rate_of_return.mean()))\n",
    "print('回测期日平均对数收益率(average log return):                 \\t{: 2.6f}'.format(df.log_return.mean()))\n",
    "\n",
    "# print('简单收益率的SR (Sharpe ratio):                 \\t{: 2.6f}'.format( s1))\n",
    "print('对数收益率的SR (Sharpe ratio):                 \\t{: 2.6f}'.format( s))\n",
    "\n",
    "# print('简单收益率的sortino比率 (sortino ratio):                 \\t{: 2.6f}'.format(sor_simpe))\n",
    "print('对数收益率的sortino比率 (sortino ratio):                 \\t{: 2.6f}'.format(sor_log))\n",
    "\n",
    "print('最大回撤MDD (max drawdown):                \\t{: 2.6%}'.format( mdd))\n",
    "print('最大回撤MDD1 (max drawdown):                \\t{: 2.6%}'.format( mdd1))\n",
    "\n",
    "print('calmar ratio (log):                \\t', calmar_log)\n",
    "\n",
    "other_metrics(df.log_return.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./resultsave/df_PPO_vit1_softmax_maxSharpe-patch3x3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['portfolio_value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTime = (end - start) // 60   # runTime是多少分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"程序运行时间为：\", runTime, \"分钟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
